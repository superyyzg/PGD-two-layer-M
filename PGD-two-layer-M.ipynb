{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_all(data_dim = 2000, training_data_num = 12000, test_data_num = 2000):\n",
    "    \n",
    "    # generate raw data\n",
    "    mean_raw = np.zeros(data_dim)\n",
    "    dev_raw = 1\n",
    "    cov_raw = dev_raw * np.identity(data_dim)\n",
    "    X_all_raw = np.random.multivariate_normal(mean_raw, cov_raw, training_data_num + test_data_num)\n",
    "    \n",
    "    # normalize to a sphere -------------------\n",
    "    X_all_sphere = np.zeros((training_data_num + test_data_num, data_dim))\n",
    "    for i in range(training_data_num + test_data_num):\n",
    "        X_all_sphere[i,:] = X_all_raw[i,:] / np.linalg.norm(X_all_raw[i,:], ord=2)\n",
    "    # -----------------------------------------\n",
    "    \n",
    "    X_all_train, X_all_test = X_all_sphere[:training_data_num, :], X_all_sphere[training_data_num:, :]\n",
    "    \n",
    "    return X_all_train, X_all_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = 50\n",
    "training_data_num = 2000\n",
    "test_data_num = 1000\n",
    "N = 8000\n",
    "\n",
    "create_new_data = True # Whether to create new data or load existing data\n",
    "save_data = True # Whether to save the data created\n",
    "label_noise = True #False # Whether to add label noise\n",
    "mu = 0\n",
    "sigma = np.sqrt(1) # mean and standard deviation of the label noise if added\n",
    "\n",
    "if create_new_data:\n",
    "    X_all_train, X_all_test = generate_X_all(data_dim, training_data_num, test_data_num)\n",
    "    X_all_train_np = X_all_train\n",
    "\n",
    "    y_dim = 1\n",
    "    A = np.random.normal(0, 1, size=(data_dim, y_dim))\n",
    "    Y_all_train = np.dot(X_all_train, A)\n",
    "    Y_all_test = np.dot(X_all_test, A)\n",
    "\n",
    "    if label_noise:\n",
    "        noise = np.random.normal(mu, sigma, (training_data_num, y_dim))\n",
    "        Y_all_train = Y_all_train + noise\n",
    "\n",
    "    print(X_all_train.shape)\n",
    "    print(X_all_test.shape)\n",
    "    print(Y_all_train.shape)\n",
    "    print(Y_all_test.shape)\n",
    "    if save_data:\n",
    "        np.save('X_all_train.npy', X_all_train_np)\n",
    "        np.save('X_all_test.npy', X_all_test)\n",
    "        np.save('Y_all_train.npy', Y_all_train)\n",
    "        np.save('Y_all_test.npy', Y_all_test)\n",
    "else:\n",
    "    X_all_train_np = np.load('X_all_train.npy')\n",
    "    X_all_test = np.load('X_all_test.npy')\n",
    "    Y_all_train = np.load('Y_all_train.npy')\n",
    "    Y_all_test = np.load('Y_all_test.npy')\n",
    "    print(X_all_train_np.shape)\n",
    "    print(X_all_test.shape)\n",
    "    print(Y_all_train.shape)\n",
    "    print(Y_all_test.shape)\n",
    "\n",
    "#generate Q\n",
    "Q1, Q2= generate_X_all(data_dim, np.rint(N/2).astype(int), np.rint(N/2).astype(int))\n",
    "Q = np.concatenate((Q1, Q2), axis=0)\n",
    "#print('Q shape is ' + str(Q.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch tensor\n",
    "X_all_train = torch.FloatTensor(X_all_train)\n",
    "X_all_test = torch.FloatTensor(X_all_test)\n",
    "Y_all_train = torch.FloatTensor(Y_all_train)\n",
    "Y_all_test = torch.FloatTensor(Y_all_test)\n",
    "Q = torch.FloatTensor(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network class\n",
    "class precond_net(torch.nn.Module):\n",
    "    def __init__(self, dim_in, num_neurons):\n",
    "        super(precond_net, self).__init__()\n",
    "        # first layer\n",
    "        self.num_neurons = num_neurons\n",
    "        self.dense_w = torch.nn.Linear(dim_in, num_neurons, bias=False)\n",
    "        \n",
    "        # activation\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        # second layer with fixed weights\n",
    "        weight_a_np = np.random.randint(2, size=(num_neurons, 1))\n",
    "        weight_a_np[np.where(weight_a_np==0)]=-1\n",
    "        weight_a_tensor = torch.from_numpy(weight_a_np)\n",
    "        weight_a_tensor = weight_a_tensor.to(torch.float)\n",
    "        self.weight_a = torch.nn.Parameter(weight_a_tensor, requires_grad=False)\n",
    "        # print(self.weight_a.shape)\n",
    "        # initialize first layer\n",
    "        self._init_linear()\n",
    "        \n",
    "    def _init_linear(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                stdv = 1. / math.sqrt(m.weight.size(1))\n",
    "                m.weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.dense_w(x)\n",
    "        y = self.relu(y)\n",
    "        y = torch.matmul(y, self.weight_a)\n",
    "        y = y/math.sqrt(self.num_neurons)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to compute the NTK\n",
    "def compute_NTK(test_data,train_data):\n",
    "    data_cor = np.matmul(test_data, np.moveaxis(train_data,-1,-2))\n",
    "    data_cor[data_cor > 1.0] = 1.0\n",
    "    data_cor[data_cor < -1.0] = -1.0\n",
    "    arccos_data_cor = np.arccos(data_cor)\n",
    "    ntk = 0.5*data_cor - 1.0/(2*np.pi)*np.multiply(data_cor,arccos_data_cor)\n",
    "    return ntk\n",
    "\n",
    "# Function to compute H matrix used in back-propagation\n",
    "def get_H(x, w, a):\n",
    "    # x: n x d\n",
    "    # w: m x d\n",
    "    # a: m x 1\n",
    "    m = w.shape[0]\n",
    "    n = x.shape[0]\n",
    "    d = x.shape[1]\n",
    "    \n",
    "    H = torch.unsqueeze(x.permute(1,0), 1) # d x 1 x n\n",
    "    H = H.expand(d, m, n) # d x m x n\n",
    "    \n",
    "    sign_matrix = torch.matmul(w, x.permute(1,0)) # m x n\n",
    "    sign = torch.where(sign_matrix >= 0, 1, 0) # m x n\n",
    "    sign = torch.mul(sign, a)\n",
    "    \n",
    "    H = torch.mul(H, sign) # d x m x n\n",
    "    H = H.permute(1,0,2)\n",
    "    H = H / (n*np.sqrt(m))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the basline model\n",
    "def train_base(model, X_train, Y_train, X_test, Y_test, hidden_unit, learning_rate = 0.1, epochs = 1000, print_freq=100):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    train_loss_log = []\n",
    "    test_loss_log = []\n",
    "    \n",
    "    train_loss_final = 0\n",
    "    test_loss_final = 0\n",
    "    ending_threshold = 0.0001\n",
    "    achieved = False\n",
    "\n",
    "    data_num = X_train.shape[0]\n",
    "    data_dim = X_train.shape[1]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        H = get_H(X_train, model.dense_w.weight.data, model.weight_a.data)\n",
    "        \n",
    "        train_out = model(X_train)\n",
    "        dy = train_out - Y_train\n",
    "        dw = torch.matmul(H, dy)\n",
    "        dw = torch.mean(dw, dim=-1)\n",
    "        \n",
    "        model.dense_w.weight.data = model.dense_w.weight.data-learning_rate * dw\n",
    "        \n",
    "        train_loss = criterion(train_out, Y_train)\n",
    "        with torch.no_grad():\n",
    "            test_out = model(X_test) \n",
    "            test_loss = criterion(test_out, Y_test)\n",
    "            if (train_loss.item()<=ending_threshold) and (achieved==False): # stop training if the loss is small enough\n",
    "                achieved=True\n",
    "                print(f' Epoch[{epoch + 1}] Training Loss: {train_loss.item():.8} Test Loss: {test_loss.item():.8}')\n",
    "                train_loss_final = train_loss.item()\n",
    "                test_loss_final = test_loss.item()\n",
    "                break\n",
    "            train_loss_log.append(train_loss.item())\n",
    "            test_loss_log.append(test_loss.item())\n",
    "            if (epoch+1) % print_freq == 0:\n",
    "                    print(f' Epoch[{epoch + 1}] Training Loss: {train_loss.item():.8} Test Loss: {test_loss.item():.8}')\n",
    "            train_loss_final = np.min(train_loss_log) # train_loss.item()\n",
    "            test_loss_final = np.min(test_loss_log)  # test_loss.item()\n",
    "    return model, train_loss_log, test_loss_log, train_loss_final, test_loss_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the kernel preconditioned model\n",
    "def train_kernel_preconditioned(H_Q, model, X_train, Y_train, X_test, Y_test, hidden_unit, learning_rate = 0.1, epochs = 1000, print_freq=100, ending_threshold = 0.0001):\n",
    "    '''\n",
    "     arguments\n",
    "            precondition_option: 'inverse' or 'regular'. 'inverse' means perform inverse computation on the kernel, and 'regular' means directly using the kernel.\n",
    "            ending_threshold: stop training when the training loss is below this threshold.\n",
    "    '''\n",
    "    criterion = nn.MSELoss()\n",
    "    train_loss_log = []\n",
    "    test_loss_log = []\n",
    "    \n",
    "    train_loss_final = 0\n",
    "    test_loss_final = 0\n",
    "    achieved = False\n",
    "\n",
    "    data_num = X_train.shape[0]\n",
    "    data_dim = X_train.shape[1]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ------------------------- Preconditioning -------------------------\n",
    "        H = get_H(X_train, model.dense_w.weight.data, model.weight_a.data)\n",
    "        neuron_num = H.shape[0]\n",
    "        dim = H.shape[1]\n",
    "        H = H.view(hidden_unit*data_dim, data_num)\n",
    "        H_Q1 = H_Q*N\n",
    "        #print(H_Q1[:,0])\n",
    "        train_out = model(X_train)\n",
    "        dy = train_out - Y_train\n",
    "        Hy = torch.matmul(H,dy)\n",
    "        Hy1 = torch.matmul(H_Q1.permute(1,0),Hy)\n",
    "        dw = torch.matmul(H_Q1,Hy1)/(N)\n",
    "        \n",
    "        dw = dw.view(neuron_num,dim)\n",
    "        #dw = torch.mean(dw, dim=-1)\n",
    "        #del H \n",
    "        model.dense_w.weight.data = model.dense_w.weight.data-learning_rate * dw\n",
    "        \n",
    "        train_loss = criterion(train_out, Y_train)\n",
    "        with torch.no_grad():\n",
    "            test_out = model(X_test) \n",
    "            test_loss = criterion(test_out, Y_test)\n",
    "            if (train_loss.item()<=ending_threshold) and (achieved==False): # stop training if the loss is small enough\n",
    "                achieved=True\n",
    "                print(f' Epoch[{epoch + 1}] Training Loss: {train_loss.item():.8} Test Loss: {test_loss.item():.8}')\n",
    "                train_loss_final = train_loss.item()\n",
    "                test_loss_final = test_loss.item()\n",
    "                break\n",
    "            train_loss_log.append(train_loss.item())\n",
    "            test_loss_log.append(test_loss.item())\n",
    "            if (epoch+1) % print_freq == 0:\n",
    "                    print(f' Epoch[{epoch + 1}] Training Loss: {train_loss.item():.8} Test Loss: {test_loss.item():.8}')\n",
    "            train_loss_final = np.min(train_loss_log) # train_loss.item()\n",
    "            test_loss_final = np.min(test_loss_log)  # test_loss.item()\n",
    "    return model, train_loss_log, test_loss_log, train_loss_final, test_loss_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_unit = 20000 #5000\n",
    "net_init = precond_net(dim_in = data_dim, num_neurons = hidden_unit).state_dict()\n",
    "H_Q = get_H(Q, net_init['dense_w.weight'], net_init['weight_a'])\n",
    "H_Q  = H_Q.view(hidden_unit*data_dim, N)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.version.cuda)\n",
    "print('GPU is available: ' + str(torch.cuda.is_available()))\n",
    "#for i in range(torch.cuda.device_count()):\n",
    "#   print(torch.cuda.get_device_properties(i).name)\n",
    "num_of_gpus = torch.cuda.device_count()\n",
    "print('number of GPUs is: '+ str(num_of_gpus))\n",
    "X_all_train = X_all_train.to(device)\n",
    "Y_all_train = Y_all_train.to(device)\n",
    "X_all_test = X_all_test.to(device)\n",
    "Y_all_test = Y_all_test.to(device)\n",
    "H_Q = H_Q.to(device)\n",
    "#print(H_Q[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform training with training data of size from `smallest_data_num` to `largest_data_num` with step `data_num_step`\n",
    "smallest_data_num = 100\n",
    "largest_data_num = 1000\n",
    "data_num_step = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_train_loss_final_all = []\n",
    "vanilla_test_loss_final_all = []\n",
    "\n",
    "\n",
    "for data_num in range(smallest_data_num, largest_data_num+data_num_step, data_num_step):\n",
    "    print(data_num)\n",
    "    net = precond_net(dim_in = data_dim, num_neurons = hidden_unit)\n",
    "    net.load_state_dict(net_init)\n",
    "    net = net.to(device)\n",
    "\n",
    "    trained_model, train_loss_log, test_loss_log, train_loss_final, test_loss_final = train_base(\n",
    "    net, X_all_train[:data_num, :], Y_all_train[:data_num, :], X_all_test[:data_num, :], Y_all_test[:data_num, :], \n",
    "    hidden_unit = hidden_unit, learning_rate = 5, epochs =5000, print_freq=100)\n",
    "\n",
    "    vanilla_train_loss_final_all.append(train_loss_final)\n",
    "    vanilla_test_loss_final_all.append(test_loss_final)\n",
    "\n",
    "    np.save('data/vanilla_train_'+str(data_num)+'.npy', np.array(train_loss_log))\n",
    "    np.save('data/vanilla_test_'+str(data_num)+'.npy', np.array(test_loss_log))\n",
    "\n",
    "    np.save('data/vanilla_train_loss_final_all.npy', np.array(vanilla_train_loss_final_all))\n",
    "    np.save('data/vanilla_test_loss_final_all.npy', np.array(vanilla_test_loss_final_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = smallest_data_num\n",
    "X_train = X_all_train[:data_num, :]\n",
    "net = precond_net(dim_in = data_dim, num_neurons = hidden_unit)\n",
    "net.load_state_dict(net_init)\n",
    "net = net.to(device)\n",
    "H = get_H(X_train, net.dense_w.weight.data, net.weight_a.data)\n",
    "print(H.shape)\n",
    "print(net.dense_w.weight.data.shape)\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preconditioned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_train_loss_final_all = []\n",
    "kernel_test_loss_final_all = []\n",
    "\n",
    "for data_num in range(smallest_data_num, largest_data_num+data_num_step, data_num_step):\n",
    "    print(data_num)\n",
    "    net = precond_net(dim_in = data_dim, num_neurons = hidden_unit)\n",
    "    net.load_state_dict(net_init)\n",
    "    net = net.to(device)\n",
    "\n",
    "    \n",
    "\n",
    "    trained_model, train_loss_log, test_loss_log, train_loss_final, test_loss_final = train_kernel_preconditioned(\n",
    "    H_Q, net, X_all_train[:data_num, :], Y_all_train[:data_num, :], X_all_test[:data_num, :], Y_all_test[:data_num, :], \n",
    "    hidden_unit=hidden_unit, learning_rate=300, epochs =5000, print_freq=100)\n",
    "\n",
    "    kernel_train_loss_final_all.append(train_loss_final)\n",
    "    kernel_test_loss_final_all.append(test_loss_final)\n",
    "\n",
    "    np.save('data/kernel_train_'+str(data_num)+'.npy', np.array(train_loss_log))\n",
    "    np.save('data/kernel_test_'+str(data_num)+'.npy', np.array(test_loss_log))\n",
    "\n",
    "    np.save('data/kernel_train_loss_final_all.npy', np.array(kernel_train_loss_final_all))\n",
    "    np.save('data/kernel_test_loss_final_all.npy', np.array(kernel_test_loss_final_all))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
